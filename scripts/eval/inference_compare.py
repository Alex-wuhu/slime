import subprocess
import time
import requests
import signal
import os


## python /workspace/slime/scripts/eval/compare.py 
MEM_FRACTION = "0.35" 

MODELS = [
    {"name": "BASE", "path": "/root/Qwen3-4B", "port": 31000},
    {"name": "FINETUNE", "path": "/root/Qwen3-4B_iter_001", "port": 31001}
]
PROMPT = "Q: æ±‚ 37 Ã— 49 ç­‰äºå¤šå°‘ï¼Ÿè¯·å±•ç¤ºæ¨ç†æ­¥éª¤ã€‚"


def kill_all():
    print("ğŸ§¹ æ¸…ç†æ—§è¿›ç¨‹...")
    # ç¡®ä¿æ€å¹²å‡€
    os.system("ps -ef | grep sglang | grep -v grep | awk '{print $2}' | xargs -r kill -9")
    time.sleep(2)

def wait_ready(port, name, log_file, timeout=120):
    url = f"http://127.0.0.1:{port}/health"
    print(f"â³ ç­‰å¾… {name} (Port {port}) å°±ç»ª...", end="", flush=True)
    
    start_time = time.time()
    while time.time() - start_time < timeout:
        # æ£€æŸ¥æ—¥å¿—æ˜¯å¦æŠ¥é”™é€€å‡º
        if os.path.exists(log_file.name):
            with open(log_file.name, 'r') as f:
                # è¯»å–æœ€å 1000 å­—èŠ‚æ£€æŸ¥æ˜¯å¦æœ‰ RuntimeError
                try:
                    f.seek(0, 2)
                    size = f.tell()
                    f.seek(max(size - 1024, 0))
                    tail = f.read()
                    if "RuntimeError" in tail or "Error:" in tail:
                        print(f"\nâŒ {name} å¯åŠ¨æŠ¥é”™ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ï¼")
                        return False
                except:
                    pass

        try:
            if requests.get(url, timeout=1).status_code == 200:
                print(" âœ… å°±ç»ª")
                return True
        except:
            pass
        time.sleep(1)
        print(".", end="", flush=True)
    
    print(f" âŒ è¶…æ—¶ï¼è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {log_file.name}")
    return False

def launch_one(model_config):
    log_filename = f"server_{model_config['name'].lower()}.log"
    print(f"\nğŸš€ å¯åŠ¨ {model_config['name']} (æ—¥å¿—: {log_filename})...")
    
    f = open(log_filename, "w")
    
    cmd = [
        "python3", "-m", "sglang.launch_server",
        "--model-path", model_config['path'],
        "--port", str(model_config['port']),
        "--mem-fraction-static", MEM_FRACTION,
        "--trust-remote-code",
        "--host", "0.0.0.0",
        # === ä¿®æ”¹ç‚¹ 2: é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé˜²æ­¢ OOM ===
        "--context-length", "8192",
        # === ä¿æŒç¦ç”¨å›¾ä¼˜åŒ–ï¼ŒåŠ å¿«å¯åŠ¨ ===
        "--disable-cuda-graph"
    ]
    
    p = subprocess.Popen(cmd, stdout=f, stderr=f, preexec_fn=os.setsid)
    return p, f

def main():
    kill_all()
    
    running_procs = []
    open_files = []
    
    try:
        # 1. å¯åŠ¨ BASE
        for m in MODELS:
            proc, log_file = launch_one(m)
            running_procs.append(proc)
            open_files.append(log_file)
            
            if not wait_ready(m['port'], m['name'], log_file):
                print(f"âš ï¸ {m['name']} å¯åŠ¨å¤±è´¥ï¼Œè„šæœ¬åœæ­¢ã€‚")
                return

        # 2. æ¨ç†
        print("\n" + "="*20 + " å¼€å§‹æ¨ç†æµ‹è¯• " + "="*20)
        for m in MODELS:
            print(f"\nğŸ”¹ æ¨¡å‹: {m['name']}")
            payload = {
                "text": PROMPT,
                "sampling_params": {"temperature": 0.2, "max_new_tokens": 256}
            }
            try:
                r = requests.post(f"http://127.0.0.1:{m['port']}/generate", json=payload, timeout=120)
                if r.status_code == 200:
                    print(f"âœ… è¾“å‡º:\n{r.json()['text']}")
                else:
                    print(f"âŒ é”™è¯¯: {r.text}")
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")

    finally:
        print("\nğŸ›‘ åœæ­¢æœåŠ¡...")
        for p in running_procs:
            try:
                os.killpg(os.getpgid(p.pid), signal.SIGTERM)
            except:
                pass
        for f in open_files:
            f.close()
        kill_all()

if __name__ == "__main__":
    main()